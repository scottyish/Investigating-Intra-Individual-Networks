{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FSL FEAT in nipype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nipype\n",
    "import nipype.interfaces.io as nio\n",
    "import nipype.interfaces.fsl as fsl\n",
    "import nipype.interfaces.ants as ants\n",
    "import nipype.pipeline.engine as pe\n",
    "import nipype.interfaces.utility as util\n",
    "import nipype.algorithms.modelgen as model\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up preprocessing\n",
    "\n",
    "basically we only need SUSAN smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general set-up\n",
    "project_folder = '/home/Public/trondheim'\n",
    "work_dir = os.path.join(project_folder, 'processing', 'nipype_workflow_folders')\n",
    "\n",
    "smoothing_fwhm = 1.5 # 4.5 # or 1.5\n",
    "t_r = 1.38\n",
    "hpcutoff = 128.   # seconds\n",
    "\n",
    "# task-specific part\n",
    "subject_ids = [str(x).zfill(3) for x in range(2, 47)]\n",
    "sessions = ['rlsat']\n",
    "tasks = ['rlsat']\n",
    "# sessions = ['rlsat']\n",
    "# tasks = ['rlsat']\n",
    "spaces = ['T1w'] #, 'MNI152NLin2009cAsym']\n",
    "\n",
    "smoothing_str = str(smoothing_fwhm).replace('.', 'p')\n",
    "## check for subject ids to run\n",
    "# subject_ids = [x for x in subject_ids if not os.path.exists(f'../derivatives/susan_smoothed_func/sub-{x}/ses-rbrevl/func/sub-{x}_ses-rbrevl_task-rb_run-1_space-T1w_desc-preproc_bold_smoothed_fwhm-{smoothing_str}.nii.gz')]\n",
    "\n",
    "# subs_ = [s.split('-')[1] for s in [x.split('/')[4] for x in sorted(glob.glob('../derivatives/fmriprep/fmriprep/sub-*/ses-sstmsit'))]]\n",
    "subs_ = [s.split('-')[1] for s in [x.split('/')[4] for x in sorted(glob.glob('../derivatives/fmriprep/fmriprep/sub-*/ses-rlsat'))]]\n",
    "# already_run = [s.split('-')[1] for s in [x.split('/')[3]for x in sorted(glob.glob('../derivatives/susan_smoothed_hp_func/sub-*/ses-sstmsit/func/*task*1p5*'))]]\n",
    "# to_run = [x for x in subs_ if x not in already_run]\n",
    "subject_ids = subs_\n",
    "# subject_ids = ['041']\n",
    "# subject_ids\n",
    "\n",
    "# subject_ids = [x for x in subject_ids if not os.path.exists(f'../derivatives/susan_smoothed_func/sub-{x}/ses-sstmsit/func/sub-{x}_ses-sstmsit_task-sst_run-1_space-T1w_desc-preproc_bold_smoothed_fwhm-{smoothing_str}.nii.gz')]\n",
    "subject_ids = [x for x in subject_ids if not os.path.exists(f'../derivatives/susan_smoothed_func/sub-{x}/ses-rlsat/func/sub-{x}_ses-rlsat_task-rlsat_run-1_space-T1w_desc-preproc_bold_smoothed_fwhm-{smoothing_str}.nii.gz')]\n",
    "subject_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject_ids = [x.split('/')[3].split('-')[1] for x in sorted(glob.glob('../sourcedata/zipdata/sub-*/ses-sstmsit'))]\n",
    "# subject_ids = [s for s in subject_ids if not os.path.exists(f'../derivatives/susan_smoothed_func/sub-{s}/ses-sstmsit/func/sub-{s}_ses-sstmsit_task-sst_run-1_space-T1w_desc-preproc_bold_smoothed_fwhm-{smoothing_str}.nii.gz')]\n",
    "# subject_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject_ids = ['002','003','004','005','006','007','008','009','010','011', '012','013','014',\n",
    "#                '015','016','017','018','019','020','023','024','025','026', '027', \n",
    "#                '028', '029','031', '032', '033', '034', '035', '037', '038', '039'\n",
    "#                '041', '042', '043', '044']\n",
    "# subject_ids = ['027','029','030','031','032']\n",
    "# subject_ids=['044']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess (i.e. smooth with SUSAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = pe.Workflow(name='smooth_wf2')\n",
    "workflow.base_dir = os.path.join(work_dir, 'smoothing_wf2')\n",
    "workflow.config = {\"execution\": {\"crashdump_dir\":os.path.join(project_folder, 'processing', 'crashdumps')}}\n",
    "\n",
    "# subjects & identity1\n",
    "identity = pe.Node(util.IdentityInterface(fields=['subject_id', 'ses', 'task', 'space']), name='identity')\n",
    "identity.iterables = [('subject_id', subject_ids),\n",
    "                      ('ses', sessions),\n",
    "                      ('task', tasks),\n",
    "                      ('space', spaces)]\n",
    "\n",
    "# file selector\n",
    "templates = {'func': os.path.join(project_folder, 'derivatives', 'fmriprep', 'fmriprep', \n",
    "                                  'sub-{subject_id}', 'ses-{ses}', 'func', \n",
    "                                  'sub-{subject_id}_ses-{ses}_task-{task}_run-*_space-{space}_desc-preproc_bold.nii.gz'),\n",
    "             'mask': os.path.join(project_folder, 'derivatives', 'fmriprep', 'fmriprep', \n",
    "                                  'sub-{subject_id}', 'ses-{ses}', 'func', \n",
    "                                  'sub-{subject_id}_ses-{ses}_task-{task}_run-*_space-{space}_desc-brain_mask.nii.gz'),\n",
    "             }\n",
    "selector = pe.Node(nio.SelectFiles(templates), name='selector')\n",
    "\n",
    "#\n",
    "workflow.connect(identity, 'subject_id', selector, 'subject_id')\n",
    "workflow.connect(identity, 'ses', selector, 'ses')\n",
    "workflow.connect(identity, 'task', selector, 'task')\n",
    "workflow.connect(identity, 'space', selector, 'space')\n",
    "\n",
    "# convert to float\n",
    "prefiltered_func_data = pe.MapNode(interface=fsl.ImageMaths(out_data_type='float',\n",
    "                                             op_string = '',\n",
    "                                             suffix='_dtype'),\n",
    "                       iterfield=['in_file'],\n",
    "                       name='convert2float', mem_gb=6)   # about 6GB in T1w space\n",
    "\n",
    "workflow.connect(selector, 'func', prefiltered_func_data, 'in_file')\n",
    "\n",
    "# Determine the 2nd and 98th percentile intensities of each functional run\n",
    "# getthresh = pe.MapNode(interface=fsl.ImageStats(op_string='-p 2 -p 98'),\n",
    "#                        iterfield = ['in_file'],\n",
    "#                        name='getthreshold')\n",
    "\n",
    "# workflow.connect(prefiltered_func_data, 'out_file', getthresh, 'in_file')\n",
    "\n",
    "# Threshold the first run of the functional data at 10% of the 98th percentile\n",
    "# threshold = pe.MapNode(interface=fsl.ImageMaths(out_data_type='char',\n",
    "#                                              suffix='_thresh'),\n",
    "#                     iterfield = ['in_file'],\n",
    "#                     name='threshold')\n",
    "\n",
    "# # Define a function to get 10% of the intensity\n",
    "# def getthreshop(thresh):\n",
    "#     return '-thr %.10f -Tmin -bin'%(0.1*thresh[0][1])\n",
    "\n",
    "# workflow.connect(prefiltered_func_data, 'out_file', threshold, 'in_file')\n",
    "# workflow.connect(getthresh, ('out_stat', getthreshop), threshold, 'op_string')\n",
    "\n",
    "# Determine the median value of the functional runs using the mask\n",
    "medianval = pe.MapNode(interface=fsl.ImageStats(op_string='-k %s -p 50'),\n",
    "                       iterfield = ['in_file','mask_file'],\n",
    "                       name='medianval')\n",
    "\n",
    "workflow.connect(prefiltered_func_data, 'out_file', medianval, 'in_file')\n",
    "workflow.connect(selector, 'mask', medianval, 'mask_file')\n",
    "\n",
    "\n",
    "# # Dilate the mask. This is the final mask for level 1.\n",
    "# dilatemask = pe.MapNode(interface=fsl.ImageMaths(suffix='_dil',\n",
    "#                                               op_string='-dilF'),\n",
    "#                     iterfield=['in_file'],\n",
    "#                      name='dilatemask')\n",
    "\n",
    "# workflow.connect(threshold, 'out_file', dilatemask, 'in_file')\n",
    "\n",
    "\n",
    "# Mask the motion corrected functional runs with the dilated mask\n",
    "prefiltered_func_data_thresh = pe.MapNode(interface=fsl.ImageMaths(suffix='_mask',\n",
    "                                                op_string='-mas'),\n",
    "                       iterfield=['in_file','in_file2'],\n",
    "                       name='apply_brain_mask')\n",
    "\n",
    "workflow.connect(prefiltered_func_data, 'out_file', prefiltered_func_data_thresh, 'in_file')\n",
    "workflow.connect(selector, 'mask', prefiltered_func_data_thresh, 'in_file2')\n",
    "\n",
    "\n",
    "# Determine the mean image from each functional run\n",
    "meanfunc2 = pe.MapNode(interface=fsl.ImageMaths(op_string='-Tmean',\n",
    "                                                suffix='_mean'),\n",
    "                       iterfield=['in_file'],\n",
    "                       name='meanfunc2')\n",
    "\n",
    "workflow.connect(prefiltered_func_data_thresh, 'out_file', meanfunc2, 'in_file')\n",
    "\n",
    "\n",
    "# Merge the median values with the mean functional images into a coupled list\n",
    "# #Yes, it is Node with iterfield! Not MapNode.\n",
    "mergenode = pe.Node(interface=util.Merge(2, axis='hstack'),\n",
    "                       iterfield=['in1','in2'],\n",
    "                       name='merge')\n",
    "\n",
    "workflow.connect(meanfunc2,'out_file', mergenode, 'in1')\n",
    "workflow.connect(medianval,'out_stat', mergenode, 'in2')\n",
    "\n",
    "\n",
    "# Smooth each run using SUSAN with the brightness threshold set to 75% of the median value for each run \n",
    "# and a mask constituting the mean functional\n",
    "smooth = pe.MapNode(interface=fsl.SUSAN(),\n",
    "                    iterfield=['in_file', 'brightness_threshold', 'usans'],\n",
    "                    name='smooth', mem_gb=10)  # reserve up to 10 GB for this node (T1w space only! otherwise use 25)\n",
    "smooth.inputs.fwhm = smoothing_fwhm\n",
    "\n",
    "\n",
    "# get brightness thresholds for SUSAN\n",
    "def getbtthresh(medianvals):\n",
    "    return [0.75*val for val in medianvals]\n",
    "\n",
    "def getusans(x):\n",
    "    ## return the mean, and 0.75* the median of the func run\n",
    "    return [[tuple([val[0],0.75*val[1]])] for val in x]\n",
    "\n",
    "workflow.connect(prefiltered_func_data_thresh, 'out_file', smooth, 'in_file')\n",
    "workflow.connect(medianval, ('out_stat', getbtthresh), smooth, 'brightness_threshold')\n",
    "workflow.connect(mergenode, ('out', getusans), smooth, 'usans')\n",
    "\n",
    "\n",
    "# Mask the smoothed data with the dilated mask\n",
    "maskfunc3 = pe.MapNode(interface=fsl.ImageMaths(suffix='_mask',\n",
    "                                                op_string='-mas'),\n",
    "                       iterfield=['in_file', 'in_file2'],\n",
    "                       name='maskfunc3', mem_gb=10)\n",
    "\n",
    "workflow.connect(smooth, 'smoothed_file', maskfunc3, 'in_file')\n",
    "workflow.connect(selector, 'mask', maskfunc3, 'in_file2')\n",
    "\n",
    "\n",
    "# Scale each volume of the run so that the median value of the run is set to 10000 (FSL convention)\n",
    "intnorm = pe.MapNode(interface=fsl.ImageMaths(suffix='_intnorm'),\n",
    "                     iterfield=['in_file','op_string'],\n",
    "                     name='intnorm', mem_gb=10)\n",
    "\n",
    "# Define a function to get the scaling factor for intensity normalization\n",
    "def getinormscale(medianvals):\n",
    "    return ['-mul %.10f'%(10000./val) for val in medianvals]\n",
    "\n",
    "workflow.connect(maskfunc3, 'out_file', intnorm, 'in_file')\n",
    "workflow.connect(medianval, ('out_stat', getinormscale), intnorm, 'op_string')\n",
    "\n",
    "\n",
    "# datasink\n",
    "ds = pe.Node(nio.DataSink(), name='datasink')\n",
    "ds.inputs.base_directory = os.path.join(project_folder, 'derivatives')\n",
    "substitutions = [('_ses_%s_space_%s_subject_id_%s_task_%s' % (ses, space, sub, task), 'sub-%s/ses-%s/func/' % (sub, ses))\n",
    "                   for space in spaces \n",
    "                   for ses in sessions \n",
    "                   for sub in subject_ids\n",
    "                   for task in tasks]\n",
    "substitutions += [('dtype_mask_smooth_mask_intnorm', 'smoothed_fwhm-' + str(smoothing_fwhm).replace('.', 'p'))]\n",
    "substitutions += [('_intnorm%d'%run, '') for run in [0,1,2]]  # get rid of subfolder per run\n",
    "substitutions += [('_highpass%d'%run, '') for run in [0,1,2]] \n",
    "ds.inputs.substitutions = substitutions\n",
    "\n",
    "workflow.connect(intnorm, 'out_file', ds, 'susan_smoothed_func')  ## smoothed functional data\n",
    "\n",
    "\n",
    "# Highpass filtering below - we *do* want to do this - or maybe not?\n",
    "# Create tempMean\n",
    "tempMean = pe.MapNode(interface=fsl.ImageMaths(op_string='-Tmean',\n",
    "                                                suffix='_mean'),\n",
    "                       iterfield=['in_file'],\n",
    "                       name='tempMean')\n",
    "\n",
    "workflow.connect(intnorm, 'out_file', tempMean, 'in_file')\n",
    "\n",
    "\n",
    "# Perform temporal highpass filtering on the data. This is the same as filtered_func_data in FSL output.\n",
    "highpass = pe.MapNode(interface=fsl.ImageMaths(op_string= '-bptf %d -1 -add'%(hpcutoff/(2*t_r)), suffix='_tempfilt'),\n",
    "                      iterfield=['in_file','in_file2'],\n",
    "                      name='highpass', mem_gb=20)\n",
    "\n",
    "workflow.connect(tempMean, 'out_file', highpass, 'in_file2')\n",
    "workflow.connect(intnorm, 'out_file', highpass, 'in_file')\n",
    "\n",
    "workflow.connect(highpass, 'out_file', ds, 'susan_smoothed_hp_func')\n",
    "# workflow.connect(maskfunc3, 'out_file', ds, 'masked_func_data')  # don't save, pointless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "workflow.run(plugin='MultiProc', plugin_args={'n_procs': 16, 'memory_gb': 150})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
