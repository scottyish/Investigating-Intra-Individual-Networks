{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "shall we try the BIDS scheme for all behavioral data as well? the HED seems quite elaborate and rather unintuitive to me: https://bids-specification.readthedocs.io/en/stable/99-appendices/03-hed.html#annotating-events-by-categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import multiprocessing as mp\n",
    "import re\n",
    "\n",
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#msit response buttons\n",
    "# r = 1, g = 2 ,y = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prepBehavior(object):\n",
    "    ''' To run, simply prepBehavior(subject_n, session_n, task_name).process()\n",
    "    \n",
    "    The idea here is to extract/generate two types of files: \n",
    "    - One used to do proper behavioral analyses/modelling later on (a csv with a row per trial and many additional columns)\n",
    "    - One used for the fMRI data analyses, which follows the bids convention (one row per event, with columns [onset, trial_type, duration])\n",
    "    \n",
    "    Which events need to be extracted for the fMRI data analyses will depend on the fMRI modelling later on. \n",
    "    Currently it extracts all event types (in the behavioral files) except for pulses; \n",
    "    but it takes no effort in categorizing stimuli or anything like that (so all stimuli types appear as \"stimulus\"). \n",
    "    The only exception are responses, which are mapped to response_left and response_right so we can easily\n",
    "    check for lateralization in M1.\n",
    "    \n",
    "    - For the learning tasks, I coded up some plots as well to do a brief visual quality check.  \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, sub, ses, task, plot=True, sourcedata_dir='../sourcedata'):\n",
    "        \n",
    "        self.sub = sub\n",
    "        self.ses = ses\n",
    "        self.task = task\n",
    "        self.plot = plot\n",
    "        \n",
    "        self.save_directories = {'events': '../derivatives/behavior/sub-{}/ses-{}/func'.format(self.sub, self.ses),\n",
    "                                 'behavior': '../derivatives/event_files/sub-{}/ses-{}/func'.format(self.sub, self.ses)}\n",
    "    \n",
    "    def make_directories(self):\n",
    "        for _, directory in self.save_directories.items():\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    def find_event_files(self):\n",
    "        #self.run_fns = sorted(glob.glob('../sourcedata/sub-{sub}/ses-{ses}/task*_data/sub-{sub}_*task-{task}*_block-*_events.tsv'.format(sub=self.sub, ses=self.ses, task=self.task)))\n",
    "        #print(f'../sourcedata/zipdata/sub-{self.sub}/ses-{self.ses}/sub-{self.sub}_ses-{self.ses}_data_tmpunzip/RESOURCES/task*_data/sub-{self.sub}_*task-{self.task}*_block-*_events.tsv')\n",
    "        self.run_fns = sorted(glob.glob(f'../sourcedata/zipdata/sub-{self.sub}/ses-{self.ses}/sub-{self.sub}_ses-{self.ses}_data_tmpunzip/sub-{self.sub}_ses-{self.ses}_data/RESOURCES/task*_data/sub-{self.sub}_*task-{self.task}*_block-*_events.tsv'))\n",
    "        \n",
    "        # sub 009 rbrevl task data saved weirdly, must manually input\n",
    "        if self.sub == '009' and self.ses == 'rbrevl':\n",
    "            if self.task == 'ReferenceBack':\n",
    "                self.run_fns = ['../sourcedata/zipdata/sub-009/ses-rbrevl/sub-009_ses-rbrevl_data_tmpunzip/sub-009_ses-rbrevl_data/RESOURCES/task_data/sub-009_ses-MRI_task-ReferenceBack_datetime-20210302-083106_events.tsv',\n",
    "                               '../sourcedata/zipdata/sub-009/ses-rbrevl/sub-009_ses-rbrevl_data_tmpunzip/sub-009_ses-rbrevl_data/RESOURCES/task_data/sub-009_ses-MRI_task-ReferenceBack_datetime-20210302-084621_events.tsv']\n",
    "            elif self.task == 'reversal-learning':\n",
    "                self.run_fns = ['../sourcedata/zipdata/sub-009/ses-rbrevl/sub-009_ses-rbrevl_data_tmpunzip/sub-009_ses-rbrevl_data/RESOURCES/task_data/sub-009_task-reversal-learning_datetime-20210302-075736_events.tsv',\n",
    "                               '../sourcedata/zipdata/sub-009/ses-rbrevl/sub-009_ses-rbrevl_data_tmpunzip/sub-009_ses-rbrevl_data/RESOURCES/task_data/sub-009_task-reversal-learning_datetime-20210302-081521_events.tsv']\n",
    "        \n",
    "        if len(self.run_fns) == 0:\n",
    "            raise IOError(\"No data file found for sub {} task {}\".format(self.sub, self.task))\n",
    "        \n",
    "        self.make_directories() # only make directories if event files are found\n",
    "        \n",
    "    def load_run_events(self):\n",
    "        ''' loads event data of all found runs '''\n",
    "        self.data = []\n",
    "        for fn in self.run_fns:\n",
    "            print(fn)\n",
    "            data = pd.read_csv(fn, sep='\\t')\n",
    "            ## NB: _events-files of blocks > 1 contain ALL data, so the event file of block 3 also contains blocks 1 and 2\n",
    "            # hence, we need to find the last pulse with phase = -1, which marks the beginning of the run\n",
    "            if self.task == 'SST':\n",
    "                \n",
    "                ### SM added this ##\n",
    "                last_block_start_idx = np.where(data['event_type'] == 'non_response_keypress')[0][-1]  # cut on last space\n",
    "                data = data.iloc[last_block_start_idx:].reset_index(drop=True)\n",
    "\n",
    "                first_pulse_idx = np.where((data['event_type'] == 'pulse') & pd.isnull(data.phase))[0][0]  # cut on first pulse, which marks the start of the block\n",
    "                data = data.iloc[first_pulse_idx:].reset_index(drop=True)\n",
    "                \n",
    "                ### which used to be this, but this forgot about the dummy volumes\n",
    "#                indx_last_block_start = np.where(np.isnan(data.phase))[0][-1]\n",
    "#                data = data.loc[indx_last_block_start:].reset_index(drop=True)\n",
    "                data['correct_response_direction'] = data.direction\n",
    "                #data = data.rename(columns={'correct_response': 'correct_response_direction'})\n",
    "                self.data.append(data)\n",
    "            elif self.task == 'MSIT':\n",
    "                \n",
    "#                 last_block_start_idx = np.where(data['event_type'] == 'non_response_keypress')[0][-1]  # cut on last space\n",
    "                last_block_start_idx = np.where((data['event_type'] == 'non_response_keypress') & (data['response'] == 'space'))[0][-1]\n",
    "                data = data.iloc[last_block_start_idx:].reset_index(drop=True)\n",
    "                first_pulse_idx = np.where((data['event_type'] == 'pulse') & pd.isnull(data.phase))[0][0]  # cut on first pulse, which marks the start of the block\n",
    "                data = data.iloc[first_pulse_idx:].reset_index(drop=True)\n",
    "                \n",
    "                ### which used to be this, but this forgot about the dummy volumes\n",
    "#                indx_last_block_start = np.where(np.isnan(data.phase))[0][-1]\n",
    "#                data = data.loc[indx_last_block_start:].reset_index(drop=True)\n",
    "#                 data['correct_response_direction'] = data.direction\n",
    "                data['correct_response_direction'] = data.correct_response\n",
    "                #data = data.rename(columns={'correct_response': 'correct_response_direction'})\n",
    "                self.data.append(data)\n",
    "                \n",
    "#                 indx_last_block_start = np.where(np.isnan(data.phase))[0][-1]\n",
    "#                 data = data.loc[indx_last_block_start:].reset_index(drop=True)\n",
    "#                 data['correct_response_direction'] = data.correct_response\n",
    "#                 #data = data.rename(columns={'correct_response': 'correct_response_direction'})\n",
    "#                 self.data.append(data)\n",
    "            else:\n",
    "                indx_last_block_start = np.where(data.phase == -1)[0][-1]\n",
    "                data = data.loc[indx_last_block_start:].reset_index(drop=True)\n",
    "                data = data.rename(columns={'correct_response': 'correct_response_direction'})\n",
    "                self.data.append(data)\n",
    "    \n",
    "    def process_learning_run(self, data):\n",
    "        \n",
    "        # collect trials: take 'response' event whenever present in that trial, otherwise take 'stimulus'\n",
    "        trials_with_responses = data.loc[(data['event_type'] == 'response') & (pd.notnull(data['rt_too_slow']))]  # the latter gets rid of secondary button presses\n",
    "        trials_with_responses_idx = trials_with_responses['trial_nr'].unique()\n",
    "\n",
    "        trials_with_stimuli_idx = data.loc[data['event_type'] == 'stimulus', 'trial_nr'].unique()\n",
    "        trials_without_responses_idx = trials_with_stimuli_idx[~np.in1d(trials_with_stimuli_idx, trials_with_responses_idx)]\n",
    "        trials_without_responses = data.loc[(data['event_type'] == 'stimulus') & (data['trial_nr'].isin(trials_without_responses_idx))]\n",
    "#         trials_without_responses = data.loc[data['trial_nr'].isin(trials_without_responses_idx)]\n",
    "\n",
    "#         trials_without_responses = data.loc[data.trial_nr.isin(trials_without_responses_idx)]\n",
    "        \n",
    "        trials_stim_response = pd.concat([trials_with_responses, trials_without_responses], axis=0).sort_values('onset')\n",
    "\n",
    "        ## useful columns\n",
    "        trials_stim_response = trials_stim_response[['block_nr', 'trial_nr', 'event_type', 'onset', 'response', 'stimulus_symbol_left', 'stimulus_symbol_right', 'correct_response_direction', 'p_win_left', 'p_win_right', 'rt', 'choice_direction', 'choice_outcome', 'cue']]\n",
    "        trials_stim_response.correct_response_direction = trials_stim_response.correct_response_direction.replace({0: 'left', 1: 'right'}).astype('category')\n",
    "        trials_stim_response.choice_direction = trials_stim_response.choice_direction.replace({0: 'left', 1: 'right'}).astype('category')\n",
    "        trials_stim_response['ease'] = np.round(np.abs(trials_stim_response['p_win_left']-trials_stim_response['p_win_right']),3) ## difficulty inverted\n",
    "        trials_stim_response['accuracy'] = trials_stim_response['choice_direction'] == trials_stim_response['correct_response_direction']\n",
    "        \n",
    "#         responses = data.loc[data.event_type == 'response', ['block_nr', 'trial_nr', 'onset', 'response', 'stimulus_symbol_left', 'stimulus_symbol_right', 'correct_response_direction', 'p_win_left', 'p_win_right', 'rt', 'choice_direction', 'choice_outcome', 'cue']]\n",
    "#         responses.correct_response_direction = responses.correct_response_direction.replace({0: 'left', 1: 'right'}).astype('category')\n",
    "#         responses.choice_direction = responses.choice_direction.replace({0: 'left', 1: 'right'}).astype('category')\n",
    "#         responses['ease'] = np.round(np.abs(responses['p_win_left']-responses['p_win_right']),3) ## difficulty inverted\n",
    "#         responses['accuracy'] = responses['choice_direction'] == responses['correct_response_direction']\n",
    "        \n",
    "        return trials_stim_response\n",
    "    \n",
    "    def process_sst_run(self, data):\n",
    "\n",
    "        data['onset'] = data['onset'] - data.loc[data['event_type']=='pulse','onset'].values[0]\n",
    "        data = data.loc[(data.event_type!='pulse')]         # remove pulses\n",
    "        data = data.loc[~pd.isnull(data['trial_nr'])]       # remove stuff without trial numbers\n",
    "        data = data.loc[data.event_type.isin(['stimulus', 'response'])]  # only stim & responses\n",
    "        data = data.loc[(data.null_trial == 0)]             # remove null trials\n",
    "        data = data.loc[(data.phase == 1)]                  # Only include responses given in phase 1\n",
    "        ## find trials with responses\n",
    "        has_response = data.groupby('trial_nr')['choice_key'].apply(lambda x: np.any(pd.notnull(x)))\n",
    "        has_response.name = 'has_response'\n",
    "        data = pd.merge(data, has_response, left_on='trial_nr', right_index=True)  # merge back in\n",
    "        # make is correct column\n",
    "        data['correct'] = np.nan\n",
    "        data.loc[(data.direction == 1) & (data.choice_key == 'r'), 'correct'] = 1\n",
    "        data.loc[(data.direction == 0) & (data.choice_key == 'b'), 'correct'] = 1\n",
    "        is_correct = data.groupby('trial_nr')['correct'].apply(lambda x: np.any(pd.notnull(x)))\n",
    "        is_correct.name = 'is_correct'\n",
    "        data = pd.merge(data, is_correct, left_on='trial_nr', right_index=True)  # merge back in\n",
    "        # categorize fs / ss /go /gf\n",
    "        data['trial_type'] = np.nan\n",
    "        data.loc[(data['stopsig_trial'] == 1) & (data['has_response'] == 1) , 'trial_type'] = 'fs'\n",
    "        data.loc[(data['stopsig_trial'] == 1) & (data['has_response'] == 0), 'trial_type'] = 'ss'\n",
    "        data.loc[(data['stopsig_trial'] == 0), 'trial_type'] = 'go'\n",
    "        data.loc[(data['stopsig_trial'] == 0) & (data['has_response'] == 0), 'trial_type'] = 'gf'\n",
    "        data['correct'] = 0\n",
    "        data.loc[(data.direction == 1) & (data.choice_key == 'r'), 'correct'] = 1\n",
    "        data.loc[(data.direction == 0) & (data.choice_key == 'b'), 'correct'] = 1\n",
    "        data.loc[(data['stopsig_trial'] == 0) & (data['has_response'] == 1) & (data['is_correct'] == False), 'trial_type'] = 'go_INC'\n",
    "        # categorize response left / response right\n",
    "        data = pd.merge(data, data.loc[data.event_type=='response', ['trial_nr', 'rt']], on='trial_nr', how='outer')\n",
    "        data.loc[(data['event_type'] == 'response') & (data.response == 'b'), 'trial_type'] = 'response_right'\n",
    "        data.loc[(data['event_type'] == 'response') & (data.response == 'r'), 'trial_type'] = 'response_left'\n",
    "        responses = data\n",
    "#         print(responses)\n",
    "        \n",
    "        return responses\n",
    "        \n",
    "    def process_msit_run(self, data):\n",
    "#         data['accuracy'] = 0\n",
    "#         data.loc[(data.correct_response == 1.0) & (data.choice_key == 'r'),'accuracy'] = 1\n",
    "#         data.loc[(data.correct_response == 2.0) & (data.choice_key == 'g'),'accuracy'] = 1\n",
    "#         data.loc[(data.correct_response == 3.0) & (data.choice_key == 'y'),'accuracy'] = 1\n",
    "        if self.sub =='025':\n",
    "            data.event_type[data.response=='b'] = 'response'\n",
    "        responses = data.loc[data.event_type == 'response', ['block_nr', 'trial_nr', 'onset', 'response', 'stimuli', 'condition', 'choice_key', 'rt','correct_response']]\n",
    "        responses.correct_response = responses.correct_response.replace({1: 'index', 2: 'middle', 3:'ring'}).astype('category')\n",
    "        if self.sub =='025':\n",
    "            print('subject 025')\n",
    "            responses.choice_key[responses.response=='b'] = 'b'\n",
    "            print(responses.choice_key)\n",
    "            responses.choice_key = responses.choice_key.replace({'g': 'index', 'y': 'middle', 'b':'ring'}).astype('category')\n",
    "        else:\n",
    "            responses.choice_key = responses.choice_key.replace({'r': 'index', 'g': 'middle', 'y':'ring'}).astype('category')\n",
    "        print(responses.correct_response)\n",
    "        print(responses.choice_key)\n",
    "        responses['accuracy'] = responses['choice_key'] == responses['correct_response']\n",
    "        \n",
    "        return responses\n",
    "    \n",
    "    def process_rb_run(self,data):\n",
    "            \n",
    "        data['WMstim'] = data['WMstim'].ffill()\n",
    "        data['stimulus'] = data['stimulus'].ffill()\n",
    "        data['color'] = data['color'].ffill()\n",
    "        \n",
    "        data['onset'] = data['onset'] - data.loc[data['event_type']=='pulse','onset'].values[0]\n",
    "   \n",
    "        data = data.loc[data.event_type.isin(['stimulus','response'])]\n",
    "        stimuli_idx = data.event_type == 'stimulus'\n",
    "\n",
    "        # Previous color?\n",
    "        data.loc[stimuli_idx,'prev_color'] = np.nan\n",
    "        data.loc[stimuli_idx,'prev_color'] = data.loc[stimuli_idx,'color'].shift(1)\n",
    "        \n",
    "        data['choice_outcome'] = np.nan\n",
    "        data.loc[(data['stimulus'] == data['WMstim']) & (data['choice_key'] == 'b'), 'choice_outcome'] = 1.0 #correct match\n",
    "        data.loc[(data['stimulus'] != data['WMstim']) & (data['choice_key'] == 'r'), 'choice_outcome'] = 1.0 #correct mismatch\n",
    "        data.loc[(data['stimulus'] != data['WMstim']) & (data['choice_key'] == 'b'), 'choice_outcome'] = 0.0 #incorrect match\n",
    "        data.loc[(data['stimulus'] == data['WMstim']) & (data['choice_key'] == 'r'), 'choice_outcome'] = 0.0 #incorrect mismatch\n",
    "\n",
    "        # Switch or repeat?\n",
    "        data.loc[stimuli_idx&(data['color'] == data['prev_color']), 'switch'] = 'repeat'\n",
    "        data.loc[stimuli_idx&(data['color'] != data['prev_color']), 'switch'] = 'switch'\n",
    "        \n",
    "        data['accuracy'] = data['choice_outcome'] == 1.0\n",
    "        data.loc[pd.isnull(data['choice_outcome']), 'accuracy'] = np.nan \n",
    "        data['accuracy'] = data['accuracy'].bfill()\n",
    "\n",
    "        data.loc[(data['trial_nr'] == 0) | (data['trial_nr'] == 129), 'accuracy'] = np.nan\n",
    "\n",
    "\n",
    "        data['trial_type'] = np.nan\n",
    "        data.loc[(data['color'] == 'red') & (data['switch'] == 'repeat') & (data['stimulus'] == data['WMstim']), 'trial_type'] = 'RefRepSame' \n",
    "        data.loc[(data['color'] == 'red') & (data['switch'] == 'repeat') & (data['stimulus'] != data['WMstim']), 'trial_type'] = 'RefRepDifferent' \n",
    "        data.loc[(data['color'] == 'red') & (data['switch'] == 'switch') & (data['stimulus'] == data['WMstim']), 'trial_type'] = 'RefSwiSame' \n",
    "        data.loc[(data['color'] == 'red') & (data['switch'] == 'switch') & (data['stimulus'] != data['WMstim']), 'trial_type'] = 'RefSwiDifferent' \n",
    "        data.loc[(data['color'] == 'blue') & (data['switch'] == 'repeat') & (data['stimulus'] == data['WMstim']), 'trial_type'] = 'ComRepSame' \n",
    "        data.loc[(data['color'] == 'blue') & (data['switch'] == 'repeat') & (data['stimulus'] != data['WMstim']), 'trial_type'] = 'ComRepDifferent' \n",
    "        data.loc[(data['color'] == 'blue') & (data['switch'] == 'switch') & (data['stimulus'] == data['WMstim']), 'trial_type'] = 'ComSwiSame' \n",
    "        data.loc[(data['color'] == 'blue') & (data['switch'] == 'switch') & (data['stimulus'] != data['WMstim']), 'trial_type'] = 'ComSwiDifferent' \n",
    "        data.loc[(data['trial_nr'] == 0) | (data['trial_nr'] == 129), 'trial_type'] = np.nan\n",
    "        #copy trial_type column for behavioral analyses (before removing trial information for incorrect trials)\n",
    "        data['condition'] = data['trial_type'].ffill()\n",
    "        data.loc[(data['trial_nr'] == 0) | (data['trial_nr'] == 129), 'condition'] = np.nan\n",
    "\n",
    "        # code error trials\n",
    "        data.loc[(data.event_type=='stimulus') & (data.accuracy == False), 'trial_type'] = 'error'\n",
    "\n",
    "    \n",
    "        data.loc[(data.event_type=='response') & (data.choice_direction == 0), 'trial_type'] = 'response_left'\n",
    "        data.loc[(data.event_type=='response') & (data.choice_direction == 1), 'trial_type'] = 'response_right'\n",
    "        \n",
    "        data['switch'] = data['switch'].ffill()\n",
    "        data['prev_color'] = data['prev_color'].ffill()\n",
    "\n",
    "        \n",
    "        # code error trials rather than left/right\n",
    "        #data.loc[(data.event_type=='response') & (data.choice_outcome == 0), 'trial_type'] = 'error'\n",
    "        #data.loc[(data.event_type=='response') & (data.choice_outcome == 1), 'trial_type'] = 'correct'\n",
    "\n",
    "\n",
    "        data = data.loc[~((pd.isnull(data['trial_type']))&(data.event_type=='response'))]\n",
    "      #  data['accuracy'] = data['choice_direction'] == data['correct_response_direction']\n",
    "       # data.loc[(data['trial_nr'] == 0) | (data['trial_nr'] == 129), 'accuracy'] = np.nan\n",
    "        data.loc[(data['trial_nr'] == 0) | (data['trial_nr'] == 129), 'choice_outcome'] = np.nan\n",
    "#         trials_stim_response['accuracy'] = trials_stim_response['choice_direction'] == trials_stim_response['correct_response_direction']\n",
    "    \n",
    "        #data['accuracy'] = np.nan\n",
    "        #data.loc[(data['choice_outcome'] == 1.0), 'accuracy'] = \n",
    "        \n",
    "        return data\n",
    "\n",
    "    def plot_learning(self, responses, f=None, ax=None):\n",
    "        if f is None:\n",
    "            f, ax = plt.subplots(2,2, figsize=(8,5))\n",
    "\n",
    "        _ = [ax_.grid() for ax_ in ax.ravel()]\n",
    "\n",
    "        all_eases = sorted(responses.ease.unique())\n",
    "        color_map = {'0.2': 'red',\n",
    "                     '0.4': 'blue',\n",
    "                     '0.6': 'green',\n",
    "                     'SPD': 'darkred',\n",
    "                     'ACC': 'darkgreen',\n",
    "                     '': 'orange'}\n",
    "        \n",
    "        all_cues = sorted(responses.cue.unique())\n",
    "        if len(all_cues) == 1:\n",
    "            all_cues = ['']\n",
    "        responses.loc[pd.isnull(responses.cue), 'cue'] = ''\n",
    "        \n",
    "        ## by cue (or overall)\n",
    "        for cue in all_cues:\n",
    "            for accuracy in [True, False]:\n",
    "                rts = responses.loc[(responses.accuracy==accuracy) & (responses.cue==cue), 'rt']\n",
    "                rts = rts * 1 if accuracy else rts*-1\n",
    "                sns.histplot(rts, color=color_map[cue], ax=ax[0,0], kde=True, label='Accuracy = {:.2f}'.format(responses.accuracy.mean()) if (accuracy and cue == 'SPD') or (accuracy and cue == '') else None)\n",
    "        ax[0,0].legend()\n",
    "        \n",
    "        for ease in all_eases:\n",
    "            color = color_map[str(ease)] # = 'blue' if ease == 0.6 else 'red'\n",
    "            for accuracy in [True, False]:\n",
    "                rts = responses.loc[(responses.ease==ease) & (responses.accuracy==accuracy), 'rt']\n",
    "                rts = rts * 1 if accuracy else rts*-1\n",
    "                sns.histplot(rts, color=color, ax=ax[1,0], kde=True, label='Ease = {}'.format(ease) if accuracy else None)\n",
    "        ax[1,0].legend()\n",
    "\n",
    "        ## quantile plots\n",
    "        qps = np.arange(.1, .91, .2)\n",
    "        for cue in all_cues:\n",
    "            quantiles = responses.loc[responses.cue==cue].groupby('accuracy')['rt'].apply(lambda x: np.quantile(x, qps))\n",
    "            acc = responses.loc[responses.cue==cue, 'accuracy'].mean()\n",
    "            if acc > 0:\n",
    "                ax[0,1].plot(quantiles.loc[True], qps*acc, linestyle='-', marker='o', color=color_map[cue], label='Accuracy = {:.2f}'.format(acc))\n",
    "            if acc < 1:\n",
    "                ax[0,1].plot(quantiles.loc[False], qps*(1-acc), linestyle='-', marker='o', color=color_map[cue])\n",
    "\n",
    "        ax[0,1].legend()\n",
    "        ax[0,1].set_ylim(0, .9)\n",
    "        ax[0,1].set_xlabel('RT')\n",
    "        ax[0,1].set_ylabel('Cum. Def. Prob.')\n",
    "\n",
    "        for ease in all_eases:\n",
    "            color = color_map[str(ease)]\n",
    "\n",
    "            acc = responses.loc[responses.ease == ease, 'accuracy'].mean()\n",
    "            quantiles = responses.loc[responses.ease == ease].groupby('accuracy')['rt'].apply(lambda x: np.quantile(x, qps))\n",
    "            #print(quantiles)\n",
    "            if acc > 0:\n",
    "                ax[1,1].plot(quantiles.loc[True], qps*acc, linestyle='-', marker='o', color=color, label='Accuracy = {:.2f}'.format(acc))\n",
    "            if acc < 1:\n",
    "                ax[1,1].plot(quantiles.loc[False], qps*(1-acc), linestyle='-', marker='o', color=color)\n",
    "            ax[1,1].set_ylim(0, .9)\n",
    "            ax[1,1].set_xlabel('RT')\n",
    "            ax[1,1].set_ylabel('Cum. Def. Prob.')\n",
    "        ax[1,1].legend()\n",
    "\n",
    "        return f, ax\n",
    "    \n",
    "    # This is where we get task-specific\n",
    "    def process(self):\n",
    "        ''' processes behavioral data '''\n",
    "        \n",
    "        print(f'Processing sub {self.sub} ses {self.ses} task {self.task}')\n",
    "        \n",
    "        self.find_event_files()\n",
    "        self.load_run_events()\n",
    "        \n",
    "        if self.task == 'ReferenceBack':\n",
    "            responses = []\n",
    "            for run, data in enumerate(self.data):\n",
    "                run += 1\n",
    "                ## subtract first pulse from onset\n",
    "                data['onset'] -= data.loc[(data.trial_nr == -1) & (data.event_type == 'pulse'), 'onset'].values[0]\n",
    "    \n",
    "#                 run_responses = self.process_rb_run(data)\n",
    "                data = run_responses = self.process_rb_run(data)\n",
    "                #data['duration'] = 0.001\n",
    "        \n",
    "                ## For fmri analyses, extract & save simple task info for now (left/right responses)\n",
    "#                 data.loc[data.event_type == 'response', 'event_type'] = data['choice_direction'].replace({0: 'response_left', 1: 'response_right'}).dropna() # get left_right responses\n",
    "#                 data = data.loc[data.event_type.isin(['stimulus', 'response_left', 'response_right']), ['onset', 'event_type', 'duration', 'trial_nr']]#.rename(columns={'event_type': 'trial_type'})\n",
    "                #data = data[['onset', 'event_type', 'trial_type', 'duration']] #.rename(columns={'event_type': 'trial_type'})\n",
    "                data = data[['onset','trial_nr','trial_type', 'duration']] # .rename(columns={'event_type': 'trial_type'})\n",
    "\n",
    "                save_fn = '../derivatives/event_files/sub-{sub}/ses-rbrevl/func/sub-{sub}_ses-{ses}_task-rb_run-{run}_events.tsv'.format(sub=self.sub, ses=self.ses, run=run)\n",
    "                data.to_csv(save_fn, sep='\\t', index=False)\n",
    "        \n",
    "                responses.append(run_responses)\n",
    "            \n",
    "            responses = pd.concat(responses)\n",
    "            save_fn = '../derivatives/behavior/sub-{sub}/ses-{ses}/func/sub-{sub}_ses-{ses}_task-{task}_behavior.tsv'.format(sub=self.sub, ses=self.ses, task=self.task)\n",
    "            responses.to_csv(save_fn, sep='\\t', index=False)\n",
    "        \n",
    "        elif self.task == 'SST':\n",
    "            responses = []\n",
    "            for run, data in enumerate(self.data):\n",
    "                run += 1\n",
    "                ## subtract first pulse from onset\n",
    "                data['onset'] -= data.loc[(np.isnan(data.trial_nr)) & (data.event_type == 'pulse'), 'onset'].values[0]\n",
    "                \n",
    "                run_responses = self.process_sst_run(data)\n",
    "\n",
    "                eventDat = run_responses\n",
    "                eventDat = eventDat.loc[eventDat.trial_type.isin(['response_left','response_right','go','fs','ss','gf','go_INC']), ['onset', 'trial_type','trial_nr','duration']]\n",
    "\n",
    "                save_fn = '../derivatives/event_files/sub-{sub}/ses-sstmsit/func/sub-{sub}_ses-{ses}_task-sst_run-{run}_events.tsv'.format(sub=self.sub, ses=self.ses, run=run)\n",
    "                eventDat.to_csv(save_fn, sep='\\t', index=False)\n",
    "        \n",
    "                # for behavioral analyses, do some other things & save more extensive things\n",
    "                responses.append(run_responses)\n",
    "                \n",
    "                if self.plot:\n",
    "                    # look at staircase of behavioural data\n",
    "                    plotDat = pd.concat(responses)\n",
    "                    tmp = plotDat.loc[(plotDat['current_ssd']>0) & (plotDat['block_nr']==run), ['trial_nr', 'staircase_id', 'current_ssd']]\n",
    "                    tmp_ssd = [k for k,_g in itertools.groupby(tmp['current_ssd'])]\n",
    "                    plt.plot(range(0,len(tmp_ssd)),tmp_ssd)\n",
    "                    plt.xlabel('Trial')\n",
    "                    plt.ylabel('SSD')\n",
    "                \n",
    "            ## Save to csv for behavior\n",
    "            responses = pd.concat(responses)\n",
    "            save_fn = '../derivatives/behavior/sub-{sub}/ses-{ses}/func/sub-{sub}_ses-{ses}_task-{task}_behavior.tsv'.format(sub=self.sub, ses=self.ses, task=self.task)\n",
    "            responses.to_csv(save_fn, sep='\\t', index=False)\n",
    "        \n",
    "        elif self.task == 'MSIT':\n",
    "            responses = []\n",
    "            for run, data in enumerate(self.data):\n",
    "                run += 1\n",
    "                ## subtract first pulse from onset\n",
    "                data['onset'] -= data.loc[(np.isnan(data.trial_nr)) & (data.event_type == 'pulse'), 'onset'].values[0]\n",
    "\n",
    "#                 if self.sub == '025':\n",
    "#                     print('processing numero 025')\n",
    "#                     data.loc[data.response == 'g', 'choice_key'] = 'r'\n",
    "#                     data.loc[data.response == 'y', 'choice_key'] = 'g'\n",
    "#                     data.loc[data.response == 'b', 'choice_key'] = 'y'\n",
    "#                     print(data['choice_key'])\n",
    "                \n",
    "                run_responses = self.process_msit_run(data)\n",
    "                \n",
    "#                 if self.sub == '025':\n",
    "#                     data.loc[data.response == 'g', 'choice_key'] = 'r'\n",
    "#                     data.loc[data.response == 'y', 'choice_key'] = 'g'\n",
    "#                     data.loc[data.response == 'b', 'choice_key'] = 'y'\n",
    "                    \n",
    "#                 else:\n",
    "                ## For fmri analyses, extract & save simple task info for now (index, middle, ring responses, conditions)\n",
    "                data['accuracy'] = 0\n",
    "                data.loc[(data.correct_response == 1.0) & (data.choice_key == 'r'),'accuracy'] = 1\n",
    "                data.loc[(data.correct_response == 2.0) & (data.choice_key == 'g'),'accuracy'] = 1\n",
    "                data.loc[(data.correct_response == 3.0) & (data.choice_key == 'y'),'accuracy'] = 1\n",
    "                data_acc = data.copy()\n",
    "                data_acc.loc[data.event_type == 'response', 'event_type'] = data['accuracy'].replace({1:'correct',0:'incorrect'}).dropna()\n",
    "                \n",
    "                data.loc[data.event_type == 'response', 'event_type'] = data['choice_key'].replace({'r': 'response_index', 'g': 'response_middle', 'y': 'response_ring'}).dropna() # get index, middle, ring responses\n",
    "                data.loc[data.event_type == 'stimulus', 'event_type'] = data['condition'].replace({1:'con',2:'simon',3:'flanker',4:'inc'}).dropna()\n",
    "                \n",
    "                data = data.loc[data.event_type.isin(['con', 'simon','flanker','inc','response_index', 'response_middle', 'response_ring', 'timing_feedback','correct','incorrect']), ['onset', 'event_type', 'duration']].rename(columns={'event_type': 'trial_type'})\n",
    "                data_acc = data_acc.loc[data_acc.event_type.isin(['correct','incorrect']), ['onset', 'event_type', 'duration']].rename(columns={'event_type': 'trial_type'})\n",
    "                data = pd.concat([data,data_acc]).sort_values(by=['onset'])\n",
    "#                 data = data.sort_values(by=['onset'])\n",
    "                \n",
    "                save_fn = '../derivatives/event_files/sub-{sub}/ses-sstmsit/func/sub-{sub}_ses-{ses}_task-msit_run-{run}_events.tsv'.format(sub=self.sub, ses=self.ses, run=run)\n",
    "                data.to_csv(save_fn, sep='\\t', index=False)\n",
    "        \n",
    "                # for behavioral analyses, do some other things & save more extensive things\n",
    "                responses.append(run_responses)\n",
    "                \n",
    "            ## Save to csv for behavior\n",
    "            responses = pd.concat(responses)\n",
    "            save_fn = '../derivatives/behavior/sub-{sub}/ses-{ses}/func/sub-{sub}_ses-{ses}_task-{task}_behavior.tsv'.format(sub=self.sub, ses=self.ses, task=self.task)\n",
    "            responses.to_csv(save_fn, sep='\\t', index=False)\n",
    "        \n",
    "        elif self.task == 'reversal-learning' or self.task == 'SAT-learning':\n",
    "            responses = []\n",
    "            for run, data in enumerate(self.data):\n",
    "                run += 1\n",
    "                ## subtract first pulse from onset\n",
    "                data['onset'] -= data.loc[(data.trial_nr == -1) & (data.event_type == 'pulse'), 'onset'].values[0]\n",
    "                \n",
    "                run_responses = self.process_learning_run(data)\n",
    "                \n",
    "                ## For fmri analyses, extract & save simple task info for now (left/right responses)\n",
    "                data.loc[data.event_type == 'response', 'event_type'] = data['choice_direction'].replace({0: 'response_left', 1: 'response_right'}).dropna() # get left_right responses\n",
    "                if not pd.isnull(data['cue']).all():\n",
    "                    data.loc[data.event_type == 'cue', 'event_type'] = data['cue'].replace({'SPD': 'cue_SPD', 'ACC': 'cue_ACC'}).dropna() # get left_right responses\n",
    "                \n",
    "                data = data.loc[data.event_type.isin(['stimulus', 'cue', 'cue_SPD', 'cue_ACC', 'response_left', 'response_right', 'feedback']), ['onset', 'event_type', 'duration', 'trial_nr']].rename(columns={'event_type': 'trial_type'})  # keep track of trial_nr!\n",
    "                \n",
    "                task_renamed = 'rlsat' if self.task == 'SAT-learning' else 'revl'\n",
    "                save_fn = '../derivatives/event_files/sub-{sub}/ses-{ses}/func/sub-{sub}_ses-{ses}_task-{task}_run-{run}_events.tsv'.format(sub=self.sub, task=task_renamed, ses=self.ses, run=run)\n",
    "                data.to_csv(save_fn, sep='\\t', index=False)\n",
    "                \n",
    "                # for behavioral analyses, do some other things & save more extensive things\n",
    "                responses.append(run_responses)\n",
    "\n",
    "            ## Save to csv for behavior\n",
    "            responses = pd.concat(responses)\n",
    "            save_fn = '../derivatives/behavior/sub-{sub}/ses-{ses}/func/sub-{sub}_ses-{ses}_task-{task}_behavior.tsv'.format(sub=self.sub, ses=self.ses, task=self.task)\n",
    "            responses.to_csv(save_fn, sep='\\t', index=False)\n",
    "            \n",
    "            ## plot?\n",
    "            if self.plot:\n",
    "                responses = responses.loc[pd.notnull(responses.rt)]\n",
    "                \n",
    "                n_runs = len(self.data)\n",
    "                subplots_heights = [1,1,.3] * (n_runs) + [1,1]\n",
    "                n_subplots_rows = len(subplots_heights)\n",
    "                ## where to plot? ignore every third, so [0,1,3,4,6,7]\n",
    "                populate_rows = np.delete(np.arange(n_subplots_rows), np.arange(2, n_subplots_rows, 3))\n",
    "                fig = plt.figure(figsize=(8,(4+n_runs*4)))\n",
    "                gs = fig.add_gridspec(n_subplots_rows, 2, height_ratios=subplots_heights, hspace=0)\n",
    "                \n",
    "                ax = np.array([[fig.add_subplot(gs[y,x]) for x in [0,1]] for y in populate_rows])\n",
    "                fig, _ = self.plot_learning(responses, f=fig, ax=ax[:2,:2])\n",
    "                for run in range(n_runs):\n",
    "                    run += 1\n",
    "                    fig, _ = self.plot_learning(responses.loc[responses.block_nr == run], f=fig, ax=ax[slice(run*2, run*2+2),:2])\n",
    "\n",
    "                # Add row titles\n",
    "                for run in range(n_runs):\n",
    "                    ax_ = fig.add_subplot(gs[(0+run*2+run):(2+run*2+run),:2])\n",
    "                    ax_.axis('off')\n",
    "                    if run == 0:\n",
    "                        ax_.set_title('Across blocks')\n",
    "                    else:\n",
    "                        ax_.set_title('Run {}'.format(run))\n",
    "                fig.tight_layout()\n",
    "                task_renamed = 'rlsat' if self.task == 'SAT-learning' else 'revl'\n",
    "                plot_save_fn = '../derivatives/quality_control_plots/sub-{sub}/ses-{ses}/func/sub-{sub}_ses-{ses}_task-{task}_desc-behavior.pdf'.format(sub=self.sub, ses=self.ses, task=task_renamed)\n",
    "#                 plot_save_fn = save_fn.replace('tsv', 'pdf').replace('behavior/', 'quality_control_plots/')\n",
    "                os.makedirs(os.path.dirname(plot_save_fn), exist_ok=True)\n",
    "                fig.savefig(plot_save_fn)\n",
    "        \n",
    "        print(f'sub {self.sub} ses {self.ses} task {self.task} processed')\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tmpunzip(sub, ses):\n",
    "\n",
    "    if not os.path.exists(f'/home/Public/trondheim/sourcedata/zipdata/sub-{sub}/ses-{ses}/sub-{sub}_ses-{ses}_data_tmpunzip'):\n",
    "        print(f'unzipping dicoms for sub {sub} ses {ses}..')\n",
    "        with zipfile.ZipFile(f'/home/Public/trondheim/sourcedata/zipdata/sub-{sub}/ses-{ses}/sub-{sub}_ses-{ses}_data.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall(f'/home/Public/trondheim/sourcedata/zipdata/sub-{sub}/ses-{ses}/sub-{sub}_ses-{ses}_data_tmpunzip')\n",
    "\n",
    "def del_tmp(sub, ses):\n",
    "    \n",
    "    if os.path.exists(f'/home/Public/trondheim/sourcedata/zipdata/sub-{sub}/ses-{ses}/sub-{sub}_ses-{ses}_data_tmpunzip'):\n",
    "        print(f'Deleting temporary dicom folder for sub-{sub} ses-{ses}')\n",
    "        shutil.rmtree(f'/home/Public/trondheim/sourcedata/zipdata/sub-{sub}/ses-{ses}/sub-{sub}_ses-{ses}_data_tmpunzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_to_process(return_tuples=False):\n",
    "    \n",
    "    all_files = sorted(glob.glob('../derivatives/bids/sub-*/ses-*/func/sub-*_ses-*_task-*_run-*.nii.gz'))\n",
    "    \n",
    "    regex = re.compile('.*sub-(?P<sub>\\d+)/ses-(?P<ses>\\S+)/func/sub-.*_ses-.*_task-(?P<task>\\S+)_run-.*_.*.nii.gz')\n",
    "    \n",
    "    all_files_dict = [regex.match(x).groupdict() for x in all_files]\n",
    "    all_files_df = pd.DataFrame.from_dict(all_files_dict).sort_values(['sub','ses','task']).drop_duplicates()\n",
    "    \n",
    "    dict_list = all_files_df.to_dict(orient='records')\n",
    "    if return_tuples:\n",
    "        return [tuple(x.values()) for x in dict_list]\n",
    "    else:\n",
    "        return dict_list\n",
    "\n",
    "\n",
    "def find_new_to_process():\n",
    "    \n",
    "    all_new_files = find_all_to_process()\n",
    "    \n",
    "    to_run = []\n",
    "    for dict_ in all_new_files:\n",
    "        fp = '../derivatives/event_files/sub-{sub}/ses-{ses}/func/sub-{sub}_ses-{ses}_task-{task}_run-1_events.tsv'.format(**dict_)\n",
    "        if not os.path.exists(fp) and not dict_['ses']=='anatomical':\n",
    "            to_run.append(dict_)\n",
    "    return [tuple(x.values()) for x in to_run]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_process = find_new_to_process()\n",
    "to_process = [x for x in to_process if x[1] == 'rlsat']\n",
    "# to_process = find_all_to_process(True)\n",
    "# to_process = [x for x in to_process if 'anatomical' not in x] # remove anatomical sessions\n",
    "#to_process = [x for x in to_process if ('025' and 'msit') not in x] # only msit sessions\n",
    "#to_process = [x for x in to_process if ('rbrevl') not in x] # only msit sessions\n",
    "to_process\n",
    "# print(to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "\n",
    "# task names in the behavioral files are not the same as in the MRI data, so we need a mapping\n",
    "task_name_mapping = {'sst': 'SST', 'msit': 'MSIT', 'rlsat': 'SAT-learning', 'rb': 'ReferenceBack', 'revl': 'reversal-learning'}\n",
    "\n",
    "for sub, ses, task in to_process:\n",
    "    \n",
    "#     if sub=='009' and ses=='rbrevl': # fix this at some point\n",
    "#         continue\n",
    "    \n",
    "    print(f'sub: {sub} ses: {ses} task: {task}')\n",
    "\n",
    "    prepBehavior(sub=sub, ses=ses, task=task_name_mapping[task]).process()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many responses per subject and condition? \n",
    "\n",
    "for sub in range(2,20):\n",
    "    print('\\n')\n",
    "    sub_id = str(sub).zfill(3)\n",
    "    print('sub-'+sub_id)\n",
    "    \n",
    "    filename = Path('../derivatives/behavior/sub-'+sub_id+'/ses-rbrevl/func/sub-'+sub_id+'_ses-rbrevl_task-ReferenceBack_behavior.tsv', sep='\\t')\n",
    "\n",
    "    if filename.exists():\n",
    "\n",
    "        dat = pd.read_csv('../derivatives/behavior/sub-'+sub_id+'/ses-rbrevl/func/sub-'+sub_id+'_ses-rbrevl_task-ReferenceBack_behavior.tsv', sep='\\t')\n",
    "        print('trials: ' +str(len(pd.unique(dat['trial_nr'])))+' \\n')\n",
    "        for trial_type in pd.unique(dat['trial_type']):\n",
    "            print(trial_type)\n",
    "            print(len(dat.loc[(dat['trial_type'] == trial_type)]))\n",
    "            \n",
    "    else:\n",
    "        print('file not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many trials per subject and condition? \n",
    "\n",
    "# if less than 64, because non-responses removed? why is number of trials still 258 then?\n",
    "\n",
    "\n",
    "for sub in range(2, len(to_process)+2):\n",
    "    print('\\n')\n",
    "    sub_id = str(sub).zfill(3)\n",
    "    print('sub-'+sub_id)\n",
    "    \n",
    "    filename = Path('../derivatives/behavior/sub-'+sub_id+'/ses-rbrevl/func/sub-'+sub_id+'_ses-rbrevl_task-ReferenceBack_behavior.tsv', sep='\\t')\n",
    "\n",
    "    if filename.exists():\n",
    "\n",
    "        dat = pd.read_csv('../derivatives/behavior/sub-'+sub_id+'/ses-rbrevl/func/sub-'+sub_id+'_ses-rbrevl_task-ReferenceBack_behavior.tsv', sep='\\t')\n",
    "        print('trials: ' +str(len(pd.unique(dat['trial_nr'])))+' \\n')\n",
    "        for trial_type in pd.unique(dat['condition']):\n",
    "            print(trial_type)\n",
    "            print(len(dat.loc[(dat['condition'] == trial_type)]))\n",
    "            \n",
    "    else:\n",
    "        print('file not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# how many erros per subject?\n",
    "\n",
    "# how many trials per subject and condition? \n",
    "\n",
    "# if less than 64, because non-responses removed? why is number of trials still 258 then?\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "for sub in range(2, len(to_process)+2):\n",
    "    #print('\\n')\n",
    "    sub_id = str(sub).zfill(3)\n",
    "    print('sub-'+sub_id)\n",
    "    filename = Path('../derivatives/behavior/sub-'+sub_id+'/ses-rbrevl/func/sub-'+sub_id+'_ses-rbrevl_task-ReferenceBack_behavior.tsv', sep='\\t')\n",
    "\n",
    "    if filename.exists():\n",
    "        dat = pd.read_csv('../derivatives/behavior/sub-'+sub_id+'/ses-rbrevl/func/sub-'+sub_id+'_ses-rbrevl_task-ReferenceBack_behavior.tsv', sep='\\t')\n",
    "        for trial_type in pd.unique(dat['trial_type']):\n",
    "            if trial_type == 'error':\n",
    "                print(len(dat.loc[(dat['trial_type'] == trial_type)]))\n",
    "                \n",
    "    else:\n",
    "        print('file not found')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepBehavior('003', 'rbrevl', 'ReferenceBack').process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepBehavior('003', 'rbrevl', 'ReferenceBack').process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepBehavior(sub='024', ses='rbrevl', task='reversal-learning').process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepBehavior(sub='002', ses='wmrevl', task='reversal-learning').process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about SSRTs in SST?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "#tmp = pd.read_csv('../derivatives/behavior/sub-002/ses-sstmsit/func/sub-002_ses-sstmsit_task-SST_behavior.tsv', sep='\\t')\n",
    "\n",
    "all_behavs = sorted(glob.glob('../derivatives/behavior/sub*/ses-sstmsit/func/sub*_ses-sstmsit_task-SST_*'))\n",
    "dfs = []\n",
    "for fn in all_behavs:\n",
    "    dfs.append(pd.read_csv(fn, sep='\\t'))\n",
    "    \n",
    "df = pd.concat(dfs)\n",
    "df = df.rename(columns={'rt_y': 'rt'})\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = df.loc[df['event_type'] == 'response']\n",
    "responses['correct_response'] = False\n",
    "responses['response_direction'] = responses.trial_type.replace({'response_left': 1, 'response_right': 0})\n",
    "responses.loc[(responses.trial_type == 'response_left') & (responses.correct_response_direction == 1), 'correct_response'] = True\n",
    "responses.loc[(responses.trial_type == 'response_right') & (responses.correct_response_direction == 0), 'correct_response'] = True\n",
    "responses\n",
    "\n",
    "df = pd.merge(left=df, right=responses[['trial_nr', 'subject', 'correct_response', 'response_direction']], on=['trial_nr', 'subject'], how='outer')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df.event_type == 'stimulus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_for_RT_idx = (((np.in1d(df['trial_type'], ['go', 'fs'])) & (df['rt'] > 1.2)) |\n",
    "                      ((np.in1d(df['trial_type'], ['go', 'fs'])) & (df['rt'] < .15)) |\n",
    "                      ((np.in1d(df['trial_type'], ['go', 'fs'])) & (pd.isnull(df['rt']))))\n",
    "print('Excluding {:.3f}% of go trials based on RT'.format(exclude_for_RT_idx.sum()/df.shape[0]))\n",
    "\n",
    "# How many errors?\n",
    "error_trial = (np.in1d(df['trial_type'], ['go', 'fs']) & (df['correct_response']==False))\n",
    "print('{:.3f}% of trials were errors'.format(error_trial.sum()/df.shape[0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['current_ssd']<0, 'current_ssd'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['subject', 'block_nr']).current_ssd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ssrt(x):\n",
    "    failed_stop_rate = np.sum(x.trial_type=='fs') / np.sum(x.stopsig_trial==1)\n",
    "    # mean SSD\n",
    "#    mean_ssd = x.loc[x.current_ssd>-1, 'ssd'].mean()/1000\n",
    "    mean_ssd = x.current_ssd.mean(skipna=True) #/1000  # to s\n",
    "    # go RT at failed_stop_rate percentile\n",
    "    go_rt_at_percentile = np.percentile(x.loc[(x.trial_type=='go') & pd.notnull(x.rt) & (x.correct_response==True), 'rt'], failed_stop_rate*100)\n",
    "    ssrt = go_rt_at_percentile - mean_ssd\n",
    "    return ssrt\n",
    "    \n",
    "def get_descriptives(x):\n",
    "    mean_go_rt = x.loc[(x.trial_type=='go') & (pd.notnull(x.rt)), 'rt'].mean()\n",
    "    median_go_rt = np.median(x.loc[(x.trial_type=='go') & (pd.notnull(x.rt)), 'rt'])\n",
    "    \n",
    "    mean_failed_stop_rt = x.loc[x.trial_type=='fs', 'rt'].mean()\n",
    "    median_failed_stop_rt = np.median(x.loc[x.trial_type=='fs', 'rt'])\n",
    "    \n",
    "    # SSRT\n",
    "    ssrt = get_ssrt(x)\n",
    "#     failed_stop_rate = np.sum(x.event_type=='failed_stop') / np.sum(x.trial_type=='stop')\n",
    "#     # mean SSD\n",
    "    \n",
    "    mean_ssd = x.current_ssd.mean(skipna=True) #/1000  # to s\n",
    "#     # go RT at failed_stop_rate percentile\n",
    "#     go_rt_at_percentile = np.percentile(x.loc[(x.trial_type=='go') & pd.notnull(x.rt) & (x.correct_response==True), 'rt'], failed_stop_rate*100)\n",
    "#     ssrt = go_rt_at_percentile - mean_ssd\n",
    "\n",
    "    # accuracy\n",
    "    accuracy = x.loc[x.trial_type == 'go', 'correct_response'].mean() #(x.trial_type=='go_trial').sum() / ( (x.event_type=='go_trial').sum() + (x.event_type=='go_error').sum() )\n",
    "    \n",
    "    failed_stop_rate = np.sum(x.trial_type=='fs') / np.sum(x.stopsig_trial==1)\n",
    "    \n",
    "    misses = np.sum((x.trial_type=='go') & (pd.isnull(x.response_direction))) / np.sum(x.trial_type=='go')\n",
    "    \n",
    "    out = pd.Series({'Median go RT': median_go_rt,\n",
    "                     'Mean go RT': mean_go_rt,\n",
    "                     'Median failed stop RT': median_failed_stop_rt,\n",
    "                     'Mean failed stop RT': mean_failed_stop_rt,\n",
    "                     'SSD': mean_ssd,\n",
    "                     'SSRT': ssrt,\n",
    "                     'Go accuracy': accuracy,\n",
    "                     'Perc. inhibition': 1-failed_stop_rate,\n",
    "                     'Misses (go)': misses})\n",
    "    return out\n",
    "    \n",
    "aggregated_per_run = df.groupby(['subject', 'block_nr']).apply(get_descriptives)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(aggregated_per_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated = df.groupby(['subject']).apply(get_descriptives).reset_index()\n",
    "\n",
    "aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated['Error rate'] = (1-aggregated['Go accuracy'])*100\n",
    "aggregated['Perc. inhibition'] *= 100\n",
    "means = aggregated.mean()\n",
    "stds = aggregated.std()\n",
    "\n",
    "\n",
    "table1 = pd.concat([means, stds], axis=1).T\n",
    "table1.index = ['mean', 'SD']\n",
    "# table1.index = ['mean', 'SD']\n",
    "\n",
    "# \n",
    "table1 = table1[['Median go RT', 'Mean go RT', 'Median failed stop RT', 'Mean failed stop RT', 'SSD', 'SSRT', 'Go accuracy', 'Perc. inhibition', 'Error rate']]\n",
    "table1\n",
    "# #table1 = pd.DataFrame(index=[0], columns=means.columns)\n",
    "# for row in range(means.shape[0]):\n",
    "#     for col in range(means.shape[1]):\n",
    "#         table1.iloc[row, col] = '{:.2f} ({:.2f})'.format(means.iloc[row, col], stds.iloc[row, col])\n",
    "# #table1 = table1.reindex(['se', 'me'])\n",
    "# table1 = table1[['Median go RT', 'Mean failed stop RT', 'Error rate', 'SSRT', 'SSD', 'Perc. inhibition']]\n",
    "# # table1.to_csv('figures/table_behavior.tsv', sep='\\t')\n",
    "# table1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
